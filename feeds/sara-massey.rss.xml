<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Understanding_Data - Sara Massey</title><link>https://SMassey25.github.io/Understanding_Data/</link><description></description><lastBuildDate>Fri, 05 Jul 2019 09:16:00 -0400</lastBuildDate><item><title>Classification Metrics - F1 Score</title><link>https://SMassey25.github.io/Understanding_Data/blog_2.html</link><description>&lt;p&gt;Classification problems predict discrete values. In this post, we will be analyzing F1 score, that uses harmonic mean as it's backbone. Before we start reading, please make sure that you understand terms like, False Positive, True Negative... Recall and Precision. If you need a refresher, check out my &lt;a href="\blog_2_prerequisites.md"&gt;previous post …&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sara Massey</dc:creator><pubDate>Fri, 05 Jul 2019 09:16:00 -0400</pubDate><guid isPermaLink="false">tag:smassey25.github.io,2019-07-05:/Understanding_Data/blog_2.html</guid><category>python</category></item><item><title>Classification Metrics - Accuracy, Precision, Recall</title><link>https://SMassey25.github.io/Understanding_Data/blog_2_prerequisites.html</link><description>&lt;p&gt;&lt;em&gt;Numbers have an important story to tell. They rely on you to give them a voice&lt;/em&gt; - &lt;strong&gt;Alex Peiniger&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once you are done with your EDA and implemented a model, we need to evaluate how well a model has performed and this is done using multiple evaluation metrics.
In this post …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sara Massey</dc:creator><pubDate>Fri, 05 Jul 2019 09:16:00 -0400</pubDate><guid isPermaLink="false">tag:smassey25.github.io,2019-07-05:/Understanding_Data/blog_2_prerequisites.html</guid><category>python</category></item><item><title>Overfitting vs Underfitting - Looking for the sweet spot</title><link>https://SMassey25.github.io/Understanding_Data/blog_1_prerequisites.html</link><description>&lt;p&gt;Generalization is a machine learning concept that evaluates how a model performs on testing or unseen data. A model could score 99% accuracy on a training model, however if it cannot replicate the accuracy results then the model is useless. Our model could either underfit or overfit in these situations …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sara Massey</dc:creator><pubDate>Thu, 13 Jun 2019 06:48:00 -0400</pubDate><guid isPermaLink="false">tag:smassey25.github.io,2019-06-13:/Understanding_Data/blog_1_prerequisites.html</guid><category>python</category></item><item><title>Getting Started with Linear Regression</title><link>https://SMassey25.github.io/Understanding_Data/blog_1.html</link><description>&lt;p&gt;On every site you go regarding data science workflow, it starts with defining the problem, gathering/cleaning and exploring the data. However, what to do after you are done with exploratory data analysis. The next two steps includes building a model and evaluating your model to answer the problem.&lt;/p&gt;
&lt;p&gt;In …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sara Massey</dc:creator><pubDate>Thu, 13 Jun 2019 04:48:00 -0400</pubDate><guid isPermaLink="false">tag:smassey25.github.io,2019-06-13:/Understanding_Data/blog_1.html</guid><category>python</category></item></channel></rss>