<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>Classification Metrics - Accuracy, Precision, Recall</title>
        <link rel="stylesheet" href="https://SMassey25.github.io/Understanding_Data/theme/css/main.css" />
        <link href="https://SMassey25.github.io/Understanding_Data/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Understanding_Data Atom Feed" />

        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="https://SMassey25.github.io/Understanding_Data/">Understanding_Data </a></h1>
                <nav><ul>
                    <li class="active"><a href="https://SMassey25.github.io/Understanding_Data/category/misc.html">misc</a></li>
                </ul>
                </nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="https://SMassey25.github.io/Understanding_Data/blog_2_prerequisites.html" rel="bookmark"
           title="Permalink to Classification Metrics - Accuracy, Precision, Recall">Classification Metrics - Accuracy, Precision, Recall</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <span>Fri 05 July 2019</span>
<span>| tags: <a href="https://SMassey25.github.io/Understanding_Data/tag/python.html">python</a></span>
</footer><!-- /.post-info -->      <p><em>Numbers have an important story to tell. They rely on you to give them a voice</em> - <strong>Alex Peiniger</strong></p>
<p>Once you are done with your EDA and implemented a model, we need to evaluate how well a model has performed and this is done using multiple evaluation metrics.
In this post, I will briefly explain how classification problems are different than regression problems. I will load the <a href="https://www.kaggle.com/uciml/mushroom-classification">Mushroom Classification</a> dataset, implement and explain multiple classification metrics such as, accuracy, precision and recall(sensitivity).</p>
<p><img alt="blog2_number.jpg" src="images\blog2_number.jpg"></p>
<h3>Regression VS Classification</h3>
<p>Machine Learning consists of many techniques to answer certain types of questions. These algorithms can be divided into the type of outputs they produce, that is, either continuous or discreate.</p>
<p>Regression models are used to predict continuous values. That is, the outputs of a regression models are not confined to a set of bins or labels. Predicting accumulated wealth based on age and education level.</p>
<p>On the other hand, classification models predict discrete values. That is, the predicted value is one of the possible outcomes in  the finite set. A classification regression that predicts one of the two values is called binary classification. Predicting <a href="https://www.youtube.com/watch?v=pqTntG1RXSY">hotdog/not hotdog</a> is an example of binary classification. In contrast, predicting one of multiple possible outcomes is called a multi-label classification. Customer segmentation is a good example of multi-label classification problem.</p>
<p><img alt="blog2_RegVClass.jpeg" src="images\blog2_RegVClass.jpeg"></p>
<p>Now that we understand the difference between regression and classification problems, let's load the mushroom classification dataset, use mapper to convert strings into numbers for the models, fit the easiest classification model aka, decision trees and finally score it on different metrics.
NOTE: Mushroom dataset is binary classification problem as we have to predict whether the mushroom is poisonous or edible.
Let's get started!</p>
<p><em>Reading the Dataset below</em></p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;mushrooms.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe" style="width:100%">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>class</th>
      <th>cap-shape</th>
      <th>cap-surface</th>
      <th>cap-color</th>
      <th>bruises</th>
      <th>odor</th>
      <th>gill-attachment</th>
      <th>gill-spacing</th>
      <th>gill-size</th>
      <th>gill-color</th>
      <th>...</th>
      <th>stalk-surface-below-ring</th>
      <th>stalk-color-above-ring</th>
      <th>stalk-color-below-ring</th>
      <th>veil-type</th>
      <th>veil-color</th>
      <th>ring-number</th>
      <th>ring-type</th>
      <th>spore-print-color</th>
      <th>population</th>
      <th>habitat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>p</td>
      <td>x</td>
      <td>s</td>
      <td>n</td>
      <td>t</td>
      <td>p</td>
      <td>f</td>
      <td>c</td>
      <td>n</td>
      <td>k</td>
      <td>...</td>
      <td>s</td>
      <td>w</td>
      <td>w</td>
      <td>p</td>
      <td>w</td>
      <td>o</td>
      <td>p</td>
      <td>k</td>
      <td>s</td>
      <td>u</td>
    </tr>
    <tr>
      <th>1</th>
      <td>e</td>
      <td>x</td>
      <td>s</td>
      <td>y</td>
      <td>t</td>
      <td>a</td>
      <td>f</td>
      <td>c</td>
      <td>b</td>
      <td>k</td>
      <td>...</td>
      <td>s</td>
      <td>w</td>
      <td>w</td>
      <td>p</td>
      <td>w</td>
      <td>o</td>
      <td>p</td>
      <td>n</td>
      <td>n</td>
      <td>g</td>
    </tr>
    <tr>
      <th>2</th>
      <td>e</td>
      <td>b</td>
      <td>s</td>
      <td>w</td>
      <td>t</td>
      <td>l</td>
      <td>f</td>
      <td>c</td>
      <td>b</td>
      <td>n</td>
      <td>...</td>
      <td>s</td>
      <td>w</td>
      <td>w</td>
      <td>p</td>
      <td>w</td>
      <td>o</td>
      <td>p</td>
      <td>n</td>
      <td>n</td>
      <td>m</td>
    </tr>
  </tbody>
</table>
<p>3 rows Ã— 23 columns</p>

</div>

<p><em>Train_Test_Split the data as it is good practice and one should do it, if possible.</em></p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;class&#39;</span><span class="p">]</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;class&#39;</span><span class="p">,</span><span class="s1">&#39;cap-color&#39;</span><span class="p">,</span><span class="s1">&#39;bruises&#39;</span><span class="p">,</span><span class="s1">&#39;odor&#39;</span><span class="p">,</span><span class="s1">&#39;ring-number&#39;</span><span class="p">,</span><span class="s1">&#39;gill-attachment&#39;</span><span class="p">,</span><span class="s1">&#39;gill-spacing&#39;</span><span class="p">,</span><span class="s1">&#39;gill-size&#39;</span><span class="p">,</span><span class="s1">&#39;gill-color&#39;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
</pre></div>


<p>In the following block of code, I have used <a href="https://github.com/scikit-learn-contrib/sklearn-pandas/blob/master/sklearn_pandas/dataframe_mapper.py">data frame mapper</a>, <a href="https://github.com/scikit-learn-contrib/sklearn-pandas/blob/master/sklearn_pandas/categorical_imputer.py">categorical imputer</a>, <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html">label binarizer</a> and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html">pipeline</a> to transform and fit the data to evaluate the model on different classification metrics. The main point of this post is to explain the classification metrics rather than transforming the data, therefore, you can skip the following piece of code.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn_pandas</span> <span class="kn">import</span> <span class="n">DataFrameMapper</span><span class="p">,</span> <span class="n">CategoricalImputer</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelBinarizer</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>


<span class="c1">#Scalling/Transforming our Features</span>
<span class="n">mapper</span> <span class="o">=</span> <span class="n">DataFrameMapper</span><span class="p">([</span>
<span class="p">(</span><span class="s1">&#39;cap-shape&#39;</span><span class="p">,[</span><span class="n">CategoricalImputer</span><span class="p">(),</span><span class="n">LabelBinarizer</span><span class="p">()]),</span>
<span class="p">(</span><span class="s1">&#39;cap-surface&#39;</span><span class="p">,[</span><span class="n">CategoricalImputer</span><span class="p">(),</span><span class="n">LabelBinarizer</span><span class="p">()]),</span>
<span class="p">(</span><span class="s1">&#39;stalk-shape&#39;</span><span class="p">,[</span><span class="n">CategoricalImputer</span><span class="p">(),</span><span class="n">LabelBinarizer</span><span class="p">()]),</span>
<span class="p">(</span><span class="s1">&#39;stalk-root&#39;</span><span class="p">,[</span><span class="n">CategoricalImputer</span><span class="p">(),</span><span class="n">LabelBinarizer</span><span class="p">()]),</span>
<span class="p">(</span><span class="s1">&#39;stalk-surface-above-ring&#39;</span><span class="p">,[</span><span class="n">CategoricalImputer</span><span class="p">(),</span><span class="n">LabelBinarizer</span><span class="p">()]),</span>
<span class="p">(</span><span class="s1">&#39;stalk-surface-below-ring&#39;</span><span class="p">,[</span><span class="n">CategoricalImputer</span><span class="p">(),</span><span class="n">LabelBinarizer</span><span class="p">()]),</span>
<span class="p">(</span><span class="s1">&#39;stalk-color-above-ring&#39;</span><span class="p">,[</span><span class="n">CategoricalImputer</span><span class="p">(),</span><span class="n">LabelBinarizer</span><span class="p">()]),</span>
<span class="p">(</span><span class="s1">&#39;stalk-color-below-ring&#39;</span><span class="p">,[</span><span class="n">CategoricalImputer</span><span class="p">(),</span><span class="n">LabelBinarizer</span><span class="p">()]),</span>
<span class="p">(</span><span class="s1">&#39;veil-type&#39;</span><span class="p">,[</span><span class="n">CategoricalImputer</span><span class="p">(),</span><span class="n">LabelBinarizer</span><span class="p">()]),</span>
<span class="p">(</span><span class="s1">&#39;veil-color&#39;</span><span class="p">,[</span><span class="n">CategoricalImputer</span><span class="p">(),</span><span class="n">LabelBinarizer</span><span class="p">()]),</span>
<span class="p">(</span><span class="s1">&#39;ring-type&#39;</span><span class="p">,[</span><span class="n">CategoricalImputer</span><span class="p">(),</span><span class="n">LabelBinarizer</span><span class="p">()]),</span>
<span class="p">(</span><span class="s1">&#39;spore-print-color&#39;</span><span class="p">,[</span><span class="n">CategoricalImputer</span><span class="p">(),</span><span class="n">LabelBinarizer</span><span class="p">()]),</span>
<span class="p">(</span><span class="s1">&#39;population&#39;</span><span class="p">,[</span><span class="n">CategoricalImputer</span><span class="p">(),</span><span class="n">LabelBinarizer</span><span class="p">()]),</span>
<span class="p">(</span><span class="s1">&#39;habitat&#39;</span><span class="p">,[</span><span class="n">CategoricalImputer</span><span class="p">(),</span><span class="n">LabelBinarizer</span><span class="p">()]),</span>
<span class="c1">#(&#39;cap-color&#39;,[CategoricalImputer(),LabelBinarizer()]),</span>
<span class="c1">#(&#39;bruises&#39;,[CategoricalImputer(),LabelBinarizer()]),</span>
<span class="c1">#(&#39;odor&#39;,[CategoricalImputer(),LabelBinarizer()]),</span>
<span class="c1">#(&#39;gill-attachment&#39;,[CategoricalImputer(),LabelBinarizer()]),</span>
<span class="c1">#(&#39;gill-spacing&#39;,[CategoricalImputer(),LabelBinarizer()]),</span>
<span class="c1">#(&#39;gill-size&#39;,[CategoricalImputer(),LabelBinarizer()]),</span>
<span class="c1">#(&#39;gill-color&#39;,[CategoricalImputer(),LabelBinarizer()]),</span>
<span class="c1">#(&#39;ring-number&#39;,[CategoricalImputer(),LabelBinarizer()]),</span>
<span class="p">],</span><span class="n">df_out</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="c1">#Pipeline to instantiate the mapper and decision tree classifier.</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">mapper</span><span class="p">,</span>
    <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="p">)</span>
<span class="c1">#Fitting our models</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="c1">#Creating predictions</span>
<span class="n">predicted</span> <span class="o">=</span> <span class="n">pipe</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">);</span>
</pre></div>


<p>Just to sum up, I implemented logistic regression in the above code and predicted values for my X_test dataset. I dropped some columns from my X_train and X_test just so that I can get more
discrepancy.</p>
<p><img alt="blog2_final.png" src="images\blog2_final.png"></p>
<h3>Confusion Matrix</h3>
<p>Confusion Matrix is not an evaluation model, however it can be used to find correctness and accuracy of a model. In addition,  most of the classification metrics are based on the terms and numbers evaluated by it. Therefore, I will implement confusion matrix and then <strong>try</strong> to explain the terms associated with it.</p>
<p><em>Note: For this example, 1 represents my positive condition that is, that the mushroom is poisonous and 0 represents my negative condition that is, the mushroom is edible. Seems weird but think in terms of disease</em></p>
<div class="highlight"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">predicted</span><span class="p">)</span>
<span class="n">cm_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;predicted poisonous&#39;</span><span class="p">,</span> <span class="s1">&#39;predicted edible&#39;</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;actual poisonous&#39;</span><span class="p">,</span> <span class="s1">&#39;actual edible&#39;</span><span class="p">])</span>
<span class="n">cm_df</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>predicted poisonous</th>
      <th>predicted edible</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>actual poisonous</th>
      <td>1013</td>
      <td>27</td>
    </tr>
    <tr>
      <th>actual edible</th>
      <td>5</td>
      <td>986</td>
    </tr>
  </tbody>
</table>
</div>

<p>To begin with, the baseline to read a confusion matrix starts by reading the diagonal from top-left to bottom-right. These two values,1013 and 986, in the above data frame are the correctly predicted values.</p>
<p><strong><em>True Positive(TP)</em></strong> values are the cases when the positive condition is predicted correctly. We could also say that the model predicted 1 and the actual value was also 1. In this example, 1013 is the True Positive value, representing the correctly predicted poisonous mushroom.</p>
<p><strong><em>True Negatives(TN)</em></strong> values are the cases when negative condition is correctly predicted. In other words, the model predicted 0 and the actual value was also 0. In this example 986 is the True Negative value, as it represents correctly edible poisonous mushrooms.</p>
<p>Now that we analyzed diagonal from top-left to bottom-right, let's look at the other diagonal. The other two values, that is 27 and 5 are called False Positive and False Negative, respectively. These terms are probably what adds confusion in the confusion matrix, so let's go.</p>
<p><strong><em>False Positive(FP)</em></strong> values are the cases when positive condition is NOT predicted correctly. That is, the model predicted 0 and the actual value was 1. In our example, 27 is the False Positive value, representing poisonous mushrooms that were predicted to be edible.</p>
<p><strong><em>False Negatives(TN)</em></strong> values are the cases when negative condition is NOT correctly predicted. In other words, the model predicted 1 and the actual value was also 0. In this example, 5 is the False Negative value, as it represents edible mushrooms that were deemed poisonous.</p>
<p><img alt="blog2_confusionMatrix.png" src="images\blog2_confusionMatrix.png"></p>
<p>Now that we sort-off understand the four terms, we can use them create multiple classification metrics such as accuracy, precision, recall and f1-score.</p>
<h3>Accuracy</h3>
<p>To begin with, accuracy accounts for the overall performance of the model. We could also say that it is a ratio of correctly predicted observations to the total observations. The formula for accuracy is following:</p>
<p>$$
Accuracy = \frac{\text{TP + TN}}{\text{TP + FP + FN + TN}} = \frac{\text{1013 + 986}}{\text{1013 + 98 + 5 + 27}} = 0.9842
$$</p>
<p>Intuitively, accuracy seems to be logical and one would think about just using this as a evaluation tool of the model. However, accuracy score falls short when there are unbalanced classes. In other words, accurancy is not reliable when there are more TP over TN or vice versa. For instance, if you have 100 credit transactions and out of those you have 5 credit fraud cases. If our model only predicts no fraud cases, our model would be 95% accurate. Therefore, accuracy should not be used when the target variables are not balanced. We calculated using the formula, now let us see how was can implement accuracy_score and get similar if not exact same score:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="n">ac</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">predicted</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
<span class="n">ac</span>
</pre></div>


<div class="highlight"><pre><span></span>0.9842442146725751
</pre></div>


<h3>Precision</h3>
<p>Precision on the other hand, only calculates how accurate the predictions regarding the positive case are(poisonous mushrooms are positive case in our example).Precision is calculated by taking the ratio of correctly predicted positive observations to the total predicted positive observations. The formula for precision is following:</p>
<p>$$
Precision = \frac{\text{TP}}{\text{TP + FP}} = \frac{\text{1013}}{\text{1013 + 27}} = 0.9740
$$</p>
<p>All in all, precision gives us the number in terms of our positive case. In the mushroom example, we are worried about the FP as marking an poisonous mushroom edible is harmful or deadly. Let us implement precision using sklearn.</p>
<p>Remember: High precision relates to the low false positive rate.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">precision_score</span>
<span class="n">pc</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">predicted</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>
<span class="n">pc</span>
</pre></div>


<div class="highlight"><pre><span></span>0.9844797253017756
</pre></div>


<h3>Recall</h3>
<p>Recall also know as sensitivity is a score for the coverage of actual positive sample. Recall is calculated by taking the ratio of correctly predicted positive cases to the all cases are in actual class. Let's look at the formula as it helped me understand the concept so it might help you too!</p>
<p>$$
Recall = \frac{\text{TP}}{\text{TP + FN}} = \frac{\text{1013}}{\text{1013 + 5}} = 0.995
$$</p>
<p>For our example, recall score would judge how well our model predicted positive cases when the actual case was positive. There is a trade-off between precision and recall score. Which evaluation metrics you use, depends on each problem.</p>
<p>Note: Specificity is another classification metrics that is the opposite of Recall.</p>
<p>Let's us implement recall from sklearn.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">recall_score</span>
<span class="n">rc</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predicted</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>
<span class="n">rc</span>
</pre></div>


<div class="highlight"><pre><span></span>0.9842442146725751
</pre></div>


<p>All in all, all these terms can be used to get understand another classification metrics, F1-Score. But I guess the blog is already too so let's keep that for another day.</p>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="social">
                        <h2>social</h2>
                        <ul>
                            <li><a href="https://SMassey25.github.io/Understanding_Data/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>

                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <p>Powered by <a href="http://getpelican.com/">Pelican</a>. Theme <a href="https://github.com/blueicefield/pelican-blueidea/">blueidea</a>, inspired by the default theme.</p>
        </footer><!-- /#contentinfo -->

</body>
</html>